{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huyen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (27455, 785). Test set shape: (7172, 785)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_p = r'C:\\Users\\huyen\\OneDrive - UNT System\\A_PhD_PATH\\COURSES\\CSCE 5218\\Project\\Computer-Vision-Project\\Computer-Vision-Project\\data\\asl_data\\sign_mnist_train.csv'\n",
    "# test_p = r'C:\\Users\\huyen\\OneDrive - UNT System\\A_PhD_PATH\\COURSES\\CSCE 5218\\Project\\Computer-Vision-Project\\Computer-Vision-Project\\data\\asl_data\\sign_mnist_valid.csv'\n",
    "train_p = r'C:\\Users\\huyen\\OneDrive - UNT System\\A_PhD_PATH\\COURSES\\CSCE 5218\\Project\\Computer-Vision-Project\\Computer-Vision-Project\\reproduced-submission\\data\\asl_data\\sign_mnist_train.csv'\n",
    "test_p = r'C:\\Users\\huyen\\OneDrive - UNT System\\A_PhD_PATH\\COURSES\\CSCE 5218\\Project\\Computer-Vision-Project\\Computer-Vision-Project\\reproduced-submission\\data\\asl_data\\sign_mnist_test.csv'\n",
    "train_data = pd.read_csv(train_p,  sep=',')\n",
    "test_data = pd.read_csv(test_p,  sep=',')\n",
    "print(f'Train set shape: {train_data.shape}. Test set shape: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     13     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the class labels to consider encoding them if neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}\n",
      "number of classes:  24\n"
     ]
    }
   ],
   "source": [
    "uniq_classes = set(list(train_data['label'])+list(test_data['label']))\n",
    "n_classes = len(uniq_classes)\n",
    "print('classes: ', uniq_classes)\n",
    "print('number of classes: ', n_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class 9 and 25 are missing. Therefore, let's reencode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23}\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {k:v for k, v in zip(list(uniq_classes), list(range(n_classes)))}\n",
    "print(label_mapping)\n",
    "\n",
    "def label_mapper(label):\n",
    "    return label_mapping[label]\n",
    "\n",
    "train_data['label'] = train_data['label'].apply(label_mapper)\n",
    "test_data['label'] = test_data['label'].apply(label_mapper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "del train_data['label']\n",
    "del test_data['label']\n",
    "\n",
    "_X_train = train_data.values.reshape(-1,28,28, 1)\n",
    "X_test = test_data.values.reshape(-1,28,28, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (21964, 28, 28, 1). y_train shape: (21964,)\n",
      "X_val shape: (5491, 28, 28, 1). y_val shape: (5491,)\n",
      "X_test shape: (7172, 28, 28, 1). y_test shape: (7172,)\n"
     ]
    }
   ],
   "source": [
    "# Spliting the training data into two sets for training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(_X_train, _y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}. y_train shape: {y_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}. y_val shape: {y_val.shape}')\n",
    "print(f'X_test shape: {X_test.shape}. y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing some image and label examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 17 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Showing image examples\n",
    "def show_imgs(feature_lists, labels):\n",
    "    for i, (features, label) in enumerate(zip (feature_lists, labels)):\n",
    "        image = features.reshape(28, 28)\n",
    "        plt.subplot(1, len(labels), i+1)\n",
    "        plt.title(label, fontdict={'fontsize': 10})\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image, cmap='gray')\n",
    "\n",
    "show_imgs(X_train[:17], y_train[:17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: min= 0, max=255\n",
      "X_train: min= 0.0, max=1.0\n",
      "X_val: min= 0.0, max=1.0\n",
      "X_test: min= 0.0, max=1.0\n"
     ]
    }
   ],
   "source": [
    "# Exploring min and max values of image pixels\n",
    "print(f'X_train: min= {X_train.min()}, max={X_train.max()}')\n",
    "\n",
    "# Normalize images by dividing pixel values by the max\n",
    "X_train = X_train/255\n",
    "X_val = X_val/255\n",
    "X_test = X_test/255\n",
    "\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "y_test = y_test.values\n",
    "\n",
    "print(f'X_train: min= {X_train.min()}, max={X_train.max()}')\n",
    "print(f'X_val: min= {X_val.min()}, max={X_val.max()}')\n",
    "print(f'X_test: min= {X_test.min()}, max={X_test.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data to Dataset and Dataloader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignALSDataset():\n",
    "    def __init__(self, X, Y) -> None:\n",
    "        super(SignALSDataset, self).__init__()\n",
    "        self.X = np.array(X, dtype=np.uint8)\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        else:\n",
    "            idx = idx\n",
    "       \n",
    "        transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                        transforms.RandomRotation(10), # randomly rotate images\n",
    "                                        transforms.RandomResizedCrop(28, scale=(0.8, 1.2)),\n",
    "                                        transforms.RandomAffine(0,translate=(0.1, 0.1)), # randomly shift images horizontally and vertically\n",
    "                                        transforms.RandomHorizontalFlip(0.1), # randomly flip images horizontally\n",
    "                                        transforms.ToTensor()]) \n",
    "        trans_x = transform(self.X[idx]).float()\n",
    "        y = torch.tensor(self.Y[idx], dtype = torch.long)\n",
    "        return trans_x, y\n",
    "        \n",
    "# passing data to the customized dataset class we created\n",
    "train_datatensor = SignALSDataset(X_train, y_train)\n",
    "val_datatensor = SignALSDataset(X_val, y_val)\n",
    "test_datatensor = SignALSDataset(X_test, y_test)\n",
    "\n",
    "# loading datasets to pytorch dataloader object\n",
    "train_dataloader = DataLoader(train_datatensor, sampler = torch.utils.data.RandomSampler(train_datatensor), batch_size=batch_size )\n",
    "val_dataloader = DataLoader(val_datatensor, sampler = torch.utils.data.SequentialSampler(val_datatensor), batch_size=batch_size )\n",
    "test_dataloader = DataLoader(test_datatensor, sampler = torch.utils.data.SequentialSampler(test_datatensor), batch_size=batch_size )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNet(\n",
      "  (conv1): Conv2d(1, 75, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batchnorm1): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(75, 50, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batchnorm2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(50, 25, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batchnorm3): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (dropout2): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=100, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=24, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    def __init__(self, n_classes) -> None:\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=75, kernel_size=3, stride=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(75)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=75, out_channels=50, kernel_size=3, stride=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(50)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=50, out_channels=25, kernel_size=3, stride=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(25)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride=1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc1 = nn.Linear(2*2*25, 512)\n",
    "        self.fc2 = nn.Linear(512, self.n_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('input x', x.size())\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool1(x)\n",
    "        # print('after block 1', x.size())\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.pool2(x)\n",
    "        # print('after block 2', x.size())\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.pool3(x)\n",
    "        # print('after block 3', x.size())\n",
    "\n",
    "        # x = torch.flatten(x, -1)\n",
    "        x = x.view(-1,2*2*25 )\n",
    "        # print('after flattening', x.size())\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "\n",
    "model= CNNNet(n_classes = n_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class CNNNet(nn.Module):\n",
    "#     def __init__(self, n_classes) -> None:\n",
    "#         super(CNNNet, self).__init__()\n",
    "#         self.n_classes = n_classes\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=1, stride=1)\n",
    "#         self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "#         self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=1, stride=1)\n",
    "#         self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "#         self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=1, stride=1)\n",
    "#         self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "#         self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "#         self.dropout1 = nn.Dropout2d(0.2)\n",
    "#         self.dropout2 = nn.Dropout2d(0.3)\n",
    "\n",
    "#         self.fc1 = nn.Linear(3*3*64, 256)\n",
    "#         self.fc2 = nn.Linear(256, self.n_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print('input x', x.size())\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = self.batchnorm1(x)\n",
    "#         x = self.pool1(x)\n",
    "#         # print('after block 1', x.size())\n",
    "\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.batchnorm2(x)\n",
    "#         x = self.pool2(x)\n",
    "#         # print('after block 2', x.size())\n",
    "\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = self.batchnorm3(x)\n",
    "#         x = self.pool3(x)\n",
    "#         # print('after block 3', x.size())\n",
    "\n",
    "#         # x = torch.flatten(x, -1)\n",
    "#         x = x.view(-1,3*3*64 )\n",
    "#         # print('after flattening', x.size())\n",
    "\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return self.softmax(x)\n",
    "\n",
    "\n",
    "# model= CNNNet(n_classes = n_classes)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 687/687 [01:15<00:00,  9.15it/s]\n",
      "100%|██████████| 172/172 [00:13<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train loss = 3.1658439414122914, Validation loss = 3.167059305102326, Validation accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 687/687 [01:21<00:00,  8.39it/s]\n",
      "100%|██████████| 172/172 [00:15<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Train loss = 3.1651969851364736, Validation loss = 3.1660241983657658, Validation accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 687/687 [01:39<00:00,  6.91it/s]\n",
      "100%|██████████| 172/172 [00:16<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: Train loss = 3.1644321563809545, Validation loss = 3.162566288959148, Validation accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 687/687 [01:36<00:00,  7.13it/s]\n",
      "100%|██████████| 172/172 [00:16<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: Train loss = 3.163833051789796, Validation loss = 3.1634914639384246, Validation accuracy: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 687/687 [01:39<00:00,  6.88it/s]\n",
      "100%|██████████| 172/172 [00:13<00:00, 12.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: Train loss = 3.162946222130388, Validation loss = 3.16339025941006, Validation accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy (y_true, y_pred):\n",
    "    return (y_pred == y_true).sum().item()/len(y_true)\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device):\n",
    "    model.train()\n",
    "    final_loss = 0.0\n",
    "    for i, data in enumerate(tqdm(data_loader, total = len(data_loader))):\n",
    "        # get input, and send to device\n",
    "        inputs, labels = data[0].to(device), data[1].to(device) #, torch.float, , dtype=torch.int64\n",
    "\n",
    "        # zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs) \n",
    "        # _, pred_labels = torch.max(outputs.data, 1)\n",
    "        pred_labels = np.argmax(outputs.data)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print loss every 1000 steps\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f'Step [{i+1}/{len(data_loader)}]: Loss = {loss.item()}, Accuracy = {get_accuracy(labels, pred_labels)}')\n",
    "\n",
    "        # save loss\n",
    "        final_loss +=loss.item()\n",
    "    return final_loss/len(data_loader)\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    final_loss = 0.0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, total = len(data_loader)):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            final_loss +=loss.item()\n",
    "\n",
    "            pred_labels = np.argmax(outputs.data) #torch.max(outputs.data, 1)\n",
    "            predictions.append(pred_labels)\n",
    "            # print('pred_labels.size(0)', pred_labels.size())\n",
    "            total+= labels.size(0)\n",
    "            correct+= get_accuracy(labels, pred_labels)\n",
    "            # print(f'correct / total: {correct} / {total}')\n",
    "\n",
    "    return final_loss/len(data_loader), correct / total, predictions\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)    \n",
    "\n",
    "train_loss_values, validation_loss_values = [], []\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_fn(train_dataloader, model, optimizer, device)\n",
    "    train_loss_values.append(train_loss)\n",
    "    \n",
    "    val_loss, val_accuracy, _ = eval_fn(val_dataloader, model)\n",
    "    validation_loss_values.append(val_loss)\n",
    "\n",
    "    # saving the best model\n",
    "    checkpoint_p = './asl_cnn_aug.pth'\n",
    "    if val_loss < best_loss:\n",
    "        torch.save(model, checkpoint_p) #.state_dict()\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    print(f'Epoch {epoch+1}/{epochs}: Train loss = {train_loss}, Validation loss = {val_loss}, Validation accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:18<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on test set: Accuracy = 0.001686245119910764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(best_model_wts)\n",
    "_, test_accuracy, test_preds = eval_fn(test_dataloader, model)\n",
    "print(f'Model performance on test set: Accuracy = {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95684bf35b52fd355f8b64198f517869e308ec096e50f26d143db610b46fcfbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
